\chapter{Eingabemethoden}
\label{cha:Eingabe}

Die meisten Interaktionen mit elektronischen Geräten jeder Art finden durch den Einsatz unserer Hände statt. Das reicht von der einfachen Bedienung von Geräten (z.B. Einschalten eines Computers) bis hin zur Steuerung von Abläufen mittels Gestik (z.B. Liederwechsel durch eine Wischgestik, die von einer Software erkannt und in einen Befehl umgewandelt wird). Doch nicht immer gibt es die Möglichkeit mit Hilfe der Hände zu interagieren. Beispielsweise sollen beim Autofahren beide Hände am Lenkrad bleiben, bei anstrengenden und präzisen Arbeiten an Maschinen müssen oft beide Hände benutzt werden oder aber auch für Menschen mit Tetraplegie bzw. Tetraparese, die ihre Hände und Arme nicht oder oft nur sehr eingeschränkt benutzen können, sind alternative Interaktionsmöglichkeiten äußerst nützlich. Aus diesem Grund beschäftigt sich dieses Kapitel mit einzelnen alternativen Eingabemethoden, die es auch ohne den Einsatz von den Händen ermöglichen mit einem System interagieren zu können. Welche Vor- und Nachteile, gesonderte Rahmenbedingungen und welche Einsatzgebiete es im Detail zu den einzelnen Methoden gibt, werden in Kapitel~\ref{cha:Vergleich} näher erläutert. 

%%%%%%%%%%%%%%%%%%%%%%% SPRACHSTEUERUNG %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sprachsteuerung}

Eine Alternative zur Interaktion mit den Händen stellt die Steuerung mit Hilfe der menschlichen Stimme dar. Ein Spracherkennungssystem soll im optimalen Fall das Gesprochene genau so gut erkennen und verstehen können wie ein Mensch. Da jeder andere Sprachangewohnheiten hat, einen unterschiedlichen Dialekt spricht und es auch syntaktische Unterschiede gibt, kann sich ein System an das menschliche Verstehen allerdings nur annähern, da diese unterschiedlichen Charakteristiken für die Interpretation erschwerend sind. Oft werden daher in der Praxis die Systeme speziell für einzelne Szenarien entwickelt bzw. angepasst. 
\newlinew \newline
Wird der Inhalt des Gesprochen analysiert und zur Interaktion verwendet, spricht man von der Spracherkennung. Hier gibt es zwei Ansätze, damit das System das Gesprochene versteht:
\begin{itemize}
      \item Mustervergleich
      \item Statistische Spracherkennung
\end{itemize}
\vspace{\baselineskip}
Bei einem Mustervergleich werden die verschieden gesprochenen Wörter verwendet und diese mit bereits abgespeicherten Mustern und Begriffen zu vergleichen. Jenes Wort, das dem Eingabewort am ähnlichsten ist, wird verwendet.
Hingegen bei der statistischen Spracherkennung werden Beschreibungen von Lauten und Wörtern verwendet. Darüber hinaus werden Statistiken verwendet, die Wahrscheinlichkeiten von Wortfolgen beinhalten \cite{KaufmannPfisterSprache}. 
\newline
\newline
%technischer aspekt
Bevor das Signal im gesamten Prozess bei dem Analyseschritt angelangt, laufen vorher noch einige Schritte ab. Wenn eine Person etwas spricht werden die analogen Wellen durch elektroakustische Wandler (zum Beispiel ein Mikrophon) in ein elektrisches Signal (Sprachsignal) umgewandelt, damit der Computer diese interpretieren und weiterverarbeiten kann. Anschließend werden auf den gespeicherten Daten verschiedenste Filter angewendet, um störende Signale, wie zum Beispiel Hintergrundgeräusche oder Pausen während des Sprechens zu entfernen. Erst dann kann im nächsten Schritt das System das Gesprochene mit Hilfe eines Mustervergleichs oder einer statistischen Spracherkennung analysieren und so den Sinn erfassen. Ist der Inhalt des Gesprochenen bekannt, kann dieser in einen Text und somit in einen Befehl umgewandelt und dadurch der Computer bzw. das Programm gesteuert werden \cite{KaufmannPfisterSprache}.
\newline \newline
Es gibt viele Faktoren, die einen Einfluss auf das Sprachsignal haben. So hat jeder Mensch neben seiner einzigartigen Stimme einen unterschiedlichen Dialekt, Sprachgewohnheiten, Emotionen in der Sprache, Pausenangewohnheiten und eine verschiedene Sprechgeschwindigkeit \cite{KaufmannPfisterSprache}. Diese Komponenten sind nicht nur wichtig für die Erkennung, was gesprochen wird, sondern viel mehr dafür, wer spricht (Sprechererkennung)
\newline \newline
%Sprechererkennung
Um festzustellen zu können welche Person gerade spricht, muss es Referenzdaten im System geben, die mit dem aktuellen Signal verglichen werden können. Dies geschieht entweder, wenn die Person ein bestimmtes Signalwort spricht, das mit dem abgespeicherten Wort verglichen wird, oder wenn das Sprachsignal lange genug ist, kann auf Grund von statistischen Merkmalen eine Übereinstimmung gefunden werden \cite{KaufmannPfisterSprache}. 
\newline \newline
%Interaktion!
Kommunikation durch Sprache ist eines der leichtesten, wenn nicht sogar intuitivsten Mittel zur Verständigung. Daher sollte auch die Interaktion mit einem Computer diesem hohen Standard entsprechen. Allerdings ist im Gegensatz zur gewohnten Face-to-Face-Kommunikation das Gegenüber im Bezug auf Spracherkennung kein intelligenter Gesprächspartner. Ein Computer kann nur schwer lange und komplizierte Sätze und Gedankengänge mitverfolgen und daraus schlecht eine Kernaussage zu ziehen. Daher muss dem System verständlich gemacht werden, wann es zuhören soll \cite{SpeechInteraction}.
Zwei gängige Methoden für den Start zur Interaktion sind:
\begin{itemize}
      \item das Drücken einer Starttaste
      \item das Aussprechen eines Aktivierungswortes
\end{itemize}
\vspace{\baselineskip}
Weil das Drücken eines Startknopfes die Hilfe der Hände benötigt, steht in diesem Zusammenhang das Aussprechen eines Aktivierungswortes zum Start der Interaktion im Vordergrund.
Durch das Wort 'Alexa' wird beispielsweise der Amazon Echo %
\footnote{https://www.amazon.com/Amazon-Echo-Bluetooth-Speaker-with-WiFi-Alexa/dp/B00X4WHP5E}
%
aktiviert. Bei Amazon Echo handelt es sich um ein Sprachinteraktionssystem, das beispielsweise Musik abspielen, den Einkauf bestellen oder auch Nachrichten und Wetterbeiträge zur Verfügung stellen kann.
Im Gegensatz zur Benutzung von Amazon Echo, muss bei der Sprachsteuerung von Apple (Siri)%
\footnote{https://www.apple.com/ios/siri/}
%
 das System erst auf die eigene Stimme eingestellt, also kalibriert werden. Das Aktivierungswort 'Hey Siri' muss drei Mal einzeln und zwei Mal in einem Satz eingesprochen werden, damit diese Funktion beispielsweise am iPhone freigeschaltet wird. Anschließend können ebenfalls Wetter oder Nachrichtendaten abgefragt werden, aber auch Anrufe oder Nachrichten können mit Hilfe der Stimme getätigt werden. 
\newline \newline
Sobald das System bereit ist den Input aufzunehmen lässt sich zusammenfassend folgendes festhalten, damit eine erfolgreiche Interaktion mit dem System geschehen kann:
\begin{itemize}
      \item keine zu große Distanz zum Mikrophon
      \item laut und deutlich zu sprechen
			\item nicht zu schnell zu sprechen
\end{itemize}
\vspace{\baselineskip}
Zusammenfassend lässt sich sagen, dass die Sprachinteraktion mit dem Computer aus Benutzersicht noch einige Verbesserung braucht, damit die Bedienung natürlicher und intuitiver wird und der einer Face-to-Face-Kommunikation näher kommt.

%%%%%%%%%%%%%%%%%%%%%%% AUGENSTEUERUNG %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Augensteuerung}
\label{cha:Augensteuerung}

Eine weitere alternative Interaktionsmöglichkeit stellt die Steuerung mit Hilfe der Augen dar. Eye-Tracking-Systeme werden verwendet um die Augenbewegungen zu verfolgen. Gleichzeitig wandelt eine Software die Augenbewegungen in einen Mauszeiger um, um so mit dem System interagieren zu können. 
\newline \newline
Um die Augenbewegungen nachvollziehen zu können, werden sowohl ein Infrarotlichtstrahl, als auch eine Videokamera auf das Auge fokussiert. Eine Software analysiert die Bewegungen im Hintergrund und kann erkennen wo der Fokus auf dem Bildschirm liegt. Für die Umrechnung ist überdies wichtig, dass die Position des Kopfes vom Interaktionssystem erkannt wird \cite{NielsenPernice}.
\newline \newline
Der Teilbereich des Auges bzw. des menschlichen Sehens, der für die Augensteuerung relevant ist, ist die Unterteilung in peripheres und foveales Sehen. Das foveale Sehen beinhaltet jenen Teil, den das menschliche Auge als scharfen Bildausschnitt wahrnimmt, also auf welchen Ausschnitt es fokussiert. Hingegen enthält das periphere Sehen den Großteil unser Sehwahrnehmung, das beinhaltet jene Bereiche, die nicht fokussiert sind \cite{NielsenPernice}. Daher ist das foveale Sehen für die Augensteuerung relevant.
\newline \newline
Nielsen und Pernice \cite{NielsenPernice} unterscheiden beim menschlichen Sehen zusätzlich zwischen Fixation und Sakkaden. Das menschliche Auge bzw. der Blick bewegt sich nicht kontinuierlich, sondern es handelt sich um es handelt sich um schelle Bewegungen mit Pausen dazwischen. Dies geschieht so schnell, dass es im Alltag nicht bewusst wahrgenommen wird. Wenn das Auge auf einem bestimmten Punkt ruht spricht man von Fixation. Die schnellen Zwischenbewegungen von einer Fixation zur nächsten werden Sakkaden genannt. Die Schwierigkeit der Eye-Tracking-Systeme und der Augensteuerungssoftware besteht darin, diese Fixationen zu erkennen, da diese meist zwischen einer hundertstel und einer zehntel Sekunde liegen \cite{NielsenPernice}.
%abbildung 2
\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{eyeTrackingSystems}
\caption{Hierachie von Eye-Tracking-Systemen \cite{Duchowski}.}
\label{fig:eyeTrackingSystems}
\end{figure}
\newline \newline
Wie in Abb.~\ref{fig:eyeTrackingSystems} dargestellt, unterscheidet Duchowski \cite{Duchowski} zwischen interaktiven und diagnostischen Systemen. Da bei diagnostischen Systemen keine direkte Interaktion stattfindet, sondern die Augenbewegungen aufgezeichnet und im Nachhinein ausgewertet werden, sind für die Analyse von Interaktionsmöglichkeiten ausschließlich interaktive Systeme relevant. 
\newline \newline
Interaktive Systeme lassen sich wiederum in zwei Bereiche teilen. Selektive Systeme benutzen den Blickpunkt als analoges Eingabegerät, während Blickkontingentsysteme für die Darstellung von komplexen Displays verwendet werden. 
\newline \newline
Abgesehen davon, wie die Daten aufgezeichnet werden bzw. was aufgezeichnet wird, ist die Unterscheidung zwischen den Eye-Tracking-Systeme relevant. Diese können in intrusive und nicht-intrusive Systeme unterteilt werden. Intrusive Eye-Tracker haben einen direkten Kontakt mit den Anwendern (z.B durch Kontaktlinsen). Nicht-intrusive Eye-Tracking-Systeme hingegen messen die Blicke mit Hilfe von einer oder mehreren Kameras. 
\newline \newline
In der Praxis werden eine Vielzahl von verschiedensten Eye-Tracking-Systemen als Basis für die Augensteuerung verwendet. Die Unternehmen Tobii %
\footnote{http://www.tobii.com/group/}
%
und SensoMotoricInsturmens (SMI) %
\footnote{https://www.smivision.com/}
%
sind Spitzenreiter auf diesem Gebiet und haben eine umfangreiche Produktpalette bzw. Anwendungsgebiete. 
\begin{figure}
\centering\small
\setlength{\tabcolsep}{0mm}	% alle Spaltenränder auf 0mm
\begin{tabular}{c@{\hspace{-15mm}}c} % mittlerer Abstand = 12mm
  \includegraphics[width=.6\textwidth]{TobiiPro_Glasses} &
  \includegraphics[width=.6\textwidth]{Tobii_Spectrum}
\\
  (a) & (b)
\\[4pt]	%vertical extra spacing (4 points)
  \includegraphics[width=.36\textwidth]{SMIRED}
\\
  (c)
\end{tabular}
%
\caption{Übersicht über die Eye-Tracking-Systeme von Tobii und SMI \newline
(a)~Tobii Pro Glasses 2 \cite{TobiiGlasses}, (b)~Tobii Pro Spectrum \cite{TobiiSpectrum} und (c)~SMI Red250mobile \cite{SMIRED}.}
\label{fig:Tobii}
\end{figure}
\newline \newline
In Abb.~\ref{fig:Tobii}(a) ist eine tragbares Eye-Tracking-System zu sehen. Dieses ist so gestaltet worden, um ein möglichst natürliches Nutzungsverhalten zu erzielen. Es müssen keine Voreinstellungen vorgenommen werden, die Brille kann sofort benutzt werden. Dieses Produkt gibt einen Einblick über das Nutzungsverhalten und weniger über die Interaktion mit einem zusätzlichen System selbst und ist daher als Beispiel für ein instrusives System angeführt. Bei Tobii Pro Spectrum (Abb.~\ref{fig:Tobii}(b)) handelt es sich um ein nicht-intrusives System, dass die Blickdaten mit einer Geschwindigkeit von bis zu 600Hz aufnimmt. Das Unternehmen SMI hat, wie in Abb.~\ref{fig:Tobii}(c) zu sehen ist, ein mobiles Eye-Tracking-System entwickelt, das die Bewegungen mit bis zu 250Hz aufzeichnen kann. 
Alle diese Systeme verwenden das Prinzip von Fixation und Sakkaden des Auges, um die Augenbewegungen nachvollziehen zu können.
\newline \newline
Bevor ein Benutzer mit Hilfe der Augen beispielsweise eine Bildschirmtastatur bedien kann, muss das System bzw. die Software erst auf den Anwender eingestellt werden. Bei der Kalibrierung tauchen am Bildschirm nacheinander Symbole auf, die fixiert werden müssen. Dadurch kann der Blickwinkel und der Augenabstand vermessen werden um so eine optimale Interaktion zu ermöglichen. 
Bei der eigentlichen Anwendung erkennt das System dann, wenn der Blick auf einem Punkt länger verharrt, also scharf stellt. Verharrt der Blick also länger auf einem Buchstaben der Tastatur wird dies als Mausklick gewertet und der Buchstabe oder das Symbol ausgewählt. Am Beispiel von Seetech von Humanelektronik %
\footnote{http://humanelektronik.de/}
%
beträgt die Fixationszeit etwa 0.5 bis 1.5 Sekunden, je nach Vertrautheit des Benutzers mit dem System \cite{SEETECH}.
\newline \newline \newline
Zusammenfassend kann gesagt werden, dass für die Interaktion mit einer Anwendung bzw. einem System interaktive Eye-Tracking-Systeme für die Nachvollziehbarkeit der Augenbewegungen und eine Software für die Umrechnung in Mausbewegungen relevant sind. Nach einer kurzen Kalibrierung kann mit der Computer durch Fixation einzelne Elemente auf dem Bildschirm auswählen, die dann auf Grund der Fixationszeit in einen Mausklick umgewandelt werden können.

%%%%%%%%%%%%%%%%%%%%%%% Gestensteuerung %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gestensteuerung}

Eine weitere alternative zur Interaktion mit den Händen stellt die Gestensteuerung% 
\footnote{Mit Hilfe von Gesten kann eine Handlung mit verschiedenen Teilen des Körpers (z.B: den Händen, Füßen, Kopf) zum Ausdruck gebracht werden. In diesem Abschnitt bezieht sich die Gestensteuerung auf alle Körperteile ausschließlich der Hände.} %
dar. Es gibt viele verschiedene Arten wie Gesten bzw. Gestik definiert werden kann. In diesem Zusammenhang versteht man unter einer Geste \cite{PreimDachselt}:
\begin{quote} ...die Bewegung von Fingern, Händen und Armen - oder auch weiterer Körperteile, wie Kopf, Augen und Lippen - aufgrund einer kommunikativen Absicht. Damit enthält die Bewegung als solche signifikante Informationen, die an den Computer übermittelt werden sollen. \end{quote}
In diesem Abschnitt werden die Kinn-, Mund-, Kopf- sowie Fußsteuerung näher erläutert. 


%%%%%%%%% Kinnsteuerung %%%%%%%%%
\subsection{Kinnsteuerung}
\label{cha:Kinnsteuerung}

Bei der Kinnsteuerung werden mit Hilfe des Kinns Bewegungen ausgeführt, um mit einem System interagieren zu können. Die unterschiedlichen Ausführungen bzw. Bedienungen werden anhand der nachfolgenden Beispiele erklärt.
%
\begin{figure}
\centering\small
\setlength{\tabcolsep}{0mm}	% alle Spaltenränder auf 0mm
\begin{tabular}{c@{\hspace{15mm}}c} % mittlerer Abstand = 12mm
  \includegraphics[width=.3\textwidth]{moso_kinn} &
  \includegraphics[width=.3\textwidth]{smilesmart_kinn}
\\
  (a) & (b)
\\[5pt]	%vertical extra spacing (4 points)
  \includegraphics[width=.3\textwidth]{sensory_kinn}
\\
  (c)
\end{tabular}
%
\caption{(a)~Moso® Kinnsteuerung \cite{MOSO}, (b)~Smile Smart Kinnsteuerung \cite{SMILESMART} und (c)~Sensory Kinnsteuerung \cite{SENSORY}.}
\label{fig:kinn}
\end{figure}
\newline \newline
Wie in Abb.~\ref{fig:kinn}(a) zu sehen ist verwendet das Unternehmen moso® %
\footnote{http://www.moso-gmbh.de/}
%
einen Joystick und zwei Tasten für die Interaktion. Um einen angenehmen Umgang zu gewährleisten, ist der Joystick mit einem kleinen weichen Ball ausgestattet. Die zwei Tasten sind links und rechts vom Joystick angebracht. Der Joystick kann sich in alle Himmelsrichtungen bewegen und kann so entweder als Mauszeiger, oder zur Steuerung eines Rollstuhls genutzt werden. Die beiden Tasten können als linke und rechte Maustasten verwendet werden \cite{MOSO}.
\newline
Abb.~\ref{fig:kinn}(b) zeigt die Version der Firma Smile Smart Technology %
\footnote{https://smilesmart-tech.com/}
%
. Diese besteht aus einem Mini Rollstuhl Joystick, der auf einer schwenkbaren Halterung montiert ist \cite{SMILESMART}.
\newline
Die Kinnsteuerung des Unternehmens Sensory Guru %
\footnote{http://www.sensoryguru.com/}
%
weist im Vergleich zu den anderen beiden keinen Ball auf. Wie in Abb.~\ref{fig:kinn}(c) dargestellt, gibt es einen Joystick mit einer Mulde für das Kinn. Mit einer Bewegung nach links oder rechts können die beiden äußeren Sensoren aktiviert werden. Das Kinn muss hier im ständigen Kontakt mit dem Ball bzw. Joystick stehen, um die Bewegungen auszuführen \cite{SENSORY}. 
\newline
Durch eine stärke Kinnbewegung nach links bzw. Rechts oder über zusätzlich angebrachte Tasten an der Steuerung, kann durch ein Menü navigiert werden oder auch andere Symbole auf einem Bildschirm ausgewählt werden. Durch die Bedienung des Joystickelementes kann der Benutzer beispielsweise die Bewegungen einer Computermaus nachahmen \cite{MOSO} \cite{SENSORY}. 
\newline
Dies kann je nach Ausführung der Steuerung verschieden viele Freiheitsgrad beinhalten. Freiheitsgrade bzw. Degrees of Freedom (DoF) bezeichnen die Bewegungsfreiheit eines Körpers. Abb.~\ref{fig:6DoF} zeigt die sechs Freiheitsgrade bzw. sechs verschiedene Bewegungsrichtungen \cite{6DoF}:
\begin{itemize}
      \item Rauf und runter 
      \item Links und rechts 
			\item Vorwärts und Rückwärts 
			\item Kippen entlang der X-Achse 
      \item Kippen entlang der Y-Achse
			\item Kippen entlang der Z-Achse
\end{itemize}
%
\vspace{\baselineskip}
Bei der Kinnsteuerung von Mosco werden fünf Freiheitsgrade ermöglichen, dass bedeutet dass der Joystick in alle Himmelsrichtungen und Neigungen bewegt werden kann, bis auf das Kippen entlang der Z-Achse. Hingegen bei der Ausführung des Unternehmens Sensory sind nur vier Freiheitsgrade möglich, da der Joystick nicht in Z-Richtung bewegt werden kann. 
Wie stark der Joystick in die bestimmte Richtung gedrückt werden muss, um eine gewisse Bewegung zu erzeugen, kann an den Einzelnen angepasst werden, um eine optimale Interaktion zu erreichen.
%
\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{6DoF}
\caption{Sechs Freiheitsgrade \cite{6DoFPic}.}
\label{fig:6DoF}
\end{figure}
%
%

%%%%%%%%% Mundsteuerung %%%%%%%%%
\subsection{Mundsteuerung}

Das Prinzip der Mundsteuerung ist ähnlich dem der Kinnsteuerung. Durch die verschiedenen Interaktionsmöglichkeiten, die durch den Einsatz des Mundes möglich sind, wird versucht eine Interaktion mit beispielsweise einem Computer möglich zu machen.
\newline \newline
Bei der in Abb.~\ref{fig:mund}(a) gezeigten Mundsteuerung handelt es sich um die IntegraMouse Plus, die das Unternehmen LIFEtool%
\footnote{http://www.lifetool.at/startseite/}
%
entwickelt hat. Die IntegraMouse Plus wird durch Nippen, Pusten und Bewegen des Mundes gesteuert. Wird die Maus mit einem Computer verbunden so kann durch ein Nippen ein Linksklick erzeugt werden, durch Pusten ein Rechtsklick und durch das Bewegen des Mundstückes kann die Richtung bzw. der Cursor verändert werden \cite{INTEGRA_VIDEO}.
\newline \newline
Ähnlich wie bei der IntegraMouse Plus wird bei dem mundgesteuerten Gamecontroller der Firma QuadStick %
\footnote{http://www.quadstick.com/}
%
, der in Abb.~\ref{fig:mund}(b) zu sehen ist, ebenfalls nippen, pusten und die Bewegungen des Mundes zur Interaktion verwendet. Dieses Modell hat allerdings anstatt eine vier Nipp- bzw. Pustsensoren, um einen regulären Gamecontroller besser imitieren zu können. Die drei zusätzlichen Sensoren können individuell angepasst werden und beispielsweise als Shortcuts für bestimmte Computerspiele verwendet werden. Ein Shortcut bzw. ein Sensor beinhaltet eine Reihenfolge von Klick- und Bewegungsabläufen, die als Kombination dargestellt werden können \cite{QUADSTICK}. In einem gewöhnlichen Computersetting könnten die zusätzlichen Sensoren beispielsweise als verschiedene Wischgesten mit einem oder mehreren Fingern verwendet werden.
\newline \newline
Bei beiden Steuerungen ist keine Kalibrierung vor der Benutzung notwendig. Es wird lediglich ein USB-Stick am Computer angeschlossen, der die Mundsteuerung per Bluetooth-Verbindung ermöglicht. 

\begin{figure}
\centering\small
\setlength{\tabcolsep}{0mm}	% alle Spaltenränder auf 0mm
\begin{tabular}{c@{\hspace{15mm}}c} % mittlerer Abstand = 12mm
  \includegraphics[width=.29\textwidth]{IntegraMouse} &
  \includegraphics[width=.4\textwidth]{quadstick}
\\
  (a) & (b)
\end{tabular}
%
\caption{(a)~IntegraMouse Plus \cite{INTEGRA} und (b)~QuadStick Gamecontroller \cite{QUADSTICK}}
\label{fig:mund}
\end{figure}

%%%%%%%%% Kopfsteuerung %%%%%%%%%
\subsection{Kopfsteuerung}

Es gibt zwei verschiedene Arten, wie man durch den Einsatz des Kopfes mit einem System interagieren kann. Zum einen kann eine Kamera verwendet werden um entweder die Bewegungen des gesamten Gesichtes, oder einen bestimmten Teil davon (z.B. den Mund) zu verfolgen und diese dann anschließend von einem System in die gewünschten Befehle umzuwandeln, andererseits kann auch eine Vorrichtung am Kopf befestigt werden und die Interaktion läuft dann haptisch ab.
\newline \newline
Bei der haptischen Variante könnte beispielsweise ein Stirnband am Kopf befestigt werden mit je einem Sensor links und rechts. Wird der Kopf in eine Richtung geneigt wird einer der beiden Sensoren aktiviert. Das Stirnband müsste dafür über Bluetooth oder WLAN mit einem Computer verbunden sein, damit eine Software die Bewegungen bzw. Sensoren ansprechen und auswerten kann. Um als Benutzer das Stirnband verwenden zu können, müsste dieses individuell angepasst werden, da es unterschiedlich ist, wie weit der Kopf zur Seite geneigt werden kann. 
\newline \newline
Bei der optischen Variante werden grundlegend zwei Arten der Kopfsteuerung bzw. der Nachvollziehbarkeit der Bewegungen unterschieden:
\begin{itemize}
      \item Gesichtserkennung
      \item Punkteverfolgung
\end{itemize}
\vspace{\baselineskip}
Bei der Gesichtserkennung wird versucht das Gesicht als Gesamtheit zu erfassen und dieses als Berechnungsgrundlage für die Bewegungen heranzuziehen. Es gibt verschiedene Arten um die Position des Kopfes zu erkennen. So kann beispielsweise die Hautfarbe dazu verwendet werden. Allerdings ist diese von Mensch zu Mensch verschieden. Des Weiteren sind bei der Berechnung bzw. Filterung viele Schritte notwendig und ist deshalb mit großem Rechenaufwand verbunden. Daher ist eine Mischung aus der haaränlichen-Gesichtserkennung und dem Camshift-Gesichtstrackingalogrythmus effizienter.
\newline \newline
Für diese sind grob vier Schritte notwendig \cite{FaceTracking}:
\begin{itemize}
      \item Als Ausgangslage wird der Unterschied vom Haaransatz zum Gesicht verwendet. Hierbei wir das Gesicht in verschiedene Rechtecke aufgeteilt. Der Unterschied wird so festgestellt, dass die Intensitätsgradienten unterschiedlich am Haar zum Gesicht sind.
      \item Zusätzlich wird im Anschluss ein Farbgesichtsdarstellungshistogramm erzeugt.
			\item Danach wird eine Gesichtswahrscheinlichkeitsberechnung für jedes Pixel durchgeführt.
			\item Abschließend werden die Größe und die Winkelberechnungen.
\end{itemize}
\vspace{\baselineskip}
Bei der Punkteverfolgung wird ein bestimmter Teilbereich des Gesichtes fokussiert, der die Bewegungsgrundlage des gesamten Kopfes darstellt. Naizhong und Jing \cite{MouthChinaControl} haben einen Algorithmus entwickelt, der die Mundbewegungen als Grundlage für die Kopfbewegungen verwendet. Dafür wird eine Kamera als Videoinput verwendet und in der Analyse können die verschiedenen Eingabebefehle unterschieden werden. So ergibt beispielsweise ein Kopfschütteln nach Links einen Linksklick oder ein zweimaliges Kopfschütteln nach Rechts einen rechten Doppelklick \cite{MouthChinaControl}.
\newline \newline
Für den Benutzer gibt es einige Punkte, die vor und während der Interaktion ausschlaggebend für eine optimale Verwendung sind. Ähnlich wie bei der Augensteuerung in Abschnitt ~\ref{cha:Augensteuerung} muss auch hier das System vorab kalibriert werden, damit eine reibungslose Interaktion möglich ist. Des Weiteren gilt es zu beachten, dass keine störenden Lichtquellen auf das Gesicht fallen und ein möglichst frontaler Blick in die Kamera gewährleistet ist \cite{MouthChinaControl}.


%%%%%%%%% Fußsteuerung %%%%%%%%%
\subsection{Fußsteuerung}

Es gibt eine Vielzahl an verschiedenen Fußsteuerungen. Manche ähneln sehr der einer Kinnsteuerung und verwenden einen Joystick, um beispielsweise Mausbewegungen zu erzeugen. Darüber hinaus werden verschiedene Tasten für die Auswahl bestimmter Elemente genutzt. Andere wiederum verwenden einen zusätzlichen Schuh bzw. eine Halterung um den Fuß, um die Fußbewegungen zu messen.
%
%
\begin{figure}
\centering\small
\setlength{\tabcolsep}{0mm}	% alle Spaltenränder auf 0mm
\begin{tabular}{c@{\hspace{0mm}}c} % mittlerer Abstand = 12mm
  \includegraphics[width=.47\textwidth]{fruewald_joystick} &
  \includegraphics[width=.47\textwidth]{fruehwald_pedal}
\\
  (a) & (b)
\\[7pt]	%vertical extra spacing (4 points)
  \includegraphics[width=.27\textwidth]{hidrex_pedal}
\\
  (c)
\end{tabular}
%
\caption{(a)~Früwald Joystick \cite{FRUEHWALD}, (b)~Frühwald Fußsteuerung \cite{FRUEHWALD} und (c)~Hidrex Fußsteuerung \cite{HIDREX}.}
\label{fig:foot}
\end{figure}
%
%
\newline
Wie in Abb.~\ref{fig:foot}(a) zu sehen ist, kann ein gewöhnlicher Joystick, der auch mit dem Kinn bedienbar ist, ebenfalls mit dem Fuß bedient werden. Der Benutzer bewegt den Joystick in die gewünschte Richtung und kann damit beispielsweise die Fahrtrichtung eines Rollstuhls oder die Mausbewegung am Computer steuern \cite{FRUEHWALD}. 
\newline
Bei der in Abb.~\ref{fig:foot}(c) dargestellten Fußsteuerung der Firma Hidrex %
\footnote{http://www.hidrex.de/}%
, handelt es sich um einen Schuhalter, der als Joystick funktioniert. Die Bedienung ist ähnlich der des herkömmlichen Joysticks. Der Fuß muss in die gewünschte Richtung gezogen bzw. bewegt werden, um einen Bewegungsablauf nachvollziehen zu können.
Die Fußsteuerung bzw. das Fußpedal des Unternehmens Frühwald%
\footnote{https://www.fruehwald.net/}
%
kann durch unterschiedlich starke Druckausübung zur Steuerung verwendet werden \cite{FRUEHWALD}. 
%
\newline \newline
Wie die einzelnen Systeme die verschiedenen Gestiken in eine Bewegung umwandeln, ist von System zu System verschieden. Aus Anwendersicht wird allerdings meist ein joystickähnliches Element für die Richtung bzw. Bewegungsrichtung bedient. Zusätzliche Tasten helfen bei der Auswahl von gewünschten Symbolen oder Elementen.

%%%%%%%%%%%%%%%%%%%%%%% Muskelsteuerung %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Muskelsteuerung}

Bei der Muskelsteuerung wird das Zusammenziehen eines Muskels oder bestimmter Muskelpartien verwendet, um so mit einem Computer interagieren zu können.
\newline \newline
Der Großteil der Muskelsteuerungssysteme verwendet zur Messung der Signale das Prinzip der Elektromyografie (EMG). Bei einer EMG-Schnittstelle handelt es sich um die Messung der Muskelkontraktion und verknüpft diese mit der gewünschten Interaktion. Die Voraussetzung an den Benutzer besteht lediglich, dass die ausgewählten Muskelpartien selbstständig angespannt bzw. aktiviert werden können. Im Zusammenhang mit der Muskelsteuerung kommt die Oberflächenelektromyografie (sEMG, englisch für Surface Electromyography) zum Einsatz. Hierbei werden die Elektroden, die für die Messung der Signale ausschlaggebend sind, an der Hauptoberfläche angebracht. Mit Hilfe der Elektroden können die myoelektrischen Signale, die die Muskeln produzieren gemessen und in einem Späteren Schritt als bestimmte Interaktion interpretiert werden \cite{EmgDefinition}.
\newline \newline
Abb.~\ref{fig:MyoAblaufOhr} zeigt die einzelnen Schritte die das Signal im System durchläuft um am Ende die Interaktion mit einem Computer zu ermöglichen. In der Studie von Barszap, Skavhaug und Joshi \cite{MyoOhr} werden zwei Frequenzbänder verwendet, um eine X- und Y- Richtung des Mauszeigers zu ermöglichen. Zieht sich der Muskel zusammen, so werden alle messbaren Signale aufgefasst und die relevanten herausgefiltert. Die Signale werden anschließend in zwei Partien aufgeteilt und durch zwei separate Bandpassfilter geschickt. \newline
Dadurch werden die gewünschten Signale bzw. Frequenzen erneut gefiltert und angepasst. Anschließend werden die beiden Signale in einen Wert zwischen 0 und 100 umgerechnet, der die Ausprägungsstärke in Prozent der jeweiligen Koordinate repräsentiert \cite{MyoOhr}.
\newline \newline
Damit ein Benutzer durch den Einsatz von myoelektrischen Signalen mit einem Computer interagieren kann, müssen zu Beginn die richtigen Positionen für die Anbringungen der Elektroden gefunden werden.  \newline \newline Es gibt verschiedene Ansätze wie die Kalibrierung im Detail aussehen kann (folgendes Beispiel bezieht sich auf eine Studie von Vernon und Joshi \cite{MyoTraining}).
%
%
\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{MyoAblaufOhr}
\caption{EMG Signalverarbeitung \cite{MyoOhr}}
\label{fig:MyoAblaufOhr}
\end{figure}
%
%
\begin{figure}
\centering\small
\setlength{\tabcolsep}{0mm}	% alle Spaltenränder auf 0mm
\begin{tabular}{c@{\hspace{15mm}}c} % mittlerer Abstand = 12mm
  \includegraphics[width=.28\textwidth]{MyoTraining1} &
  \includegraphics[width=.28\textwidth]{MyoTraining2}
\\
  (a) & (b)
\end{tabular}
%
\caption{Trainingsoberflächen: \newline
1D-Steuerung \cite{MyoTraining}~(a) und 2D-Steuerung \cite{MyoTraining}~(b)}
\label{fig:MyoTraining}
\end{figure}
%
%
\newline
Dazu werden die Elektroden an der vermuteten Stelle der zu messenden Muskeln angebracht und mit dem Computer verbunden. Auf dem Bildschirm ist das aktuelle sEMG-Signal zu sehen. Ist das Signal mit vielen Störfaktoren, dem sog. Rauschen verbunden, müssen die Elektroden gereinigt oder in der Regel neu positioniert werden. Ist die optimale Position gefunden worden, muss das System auf die individuellen Muskelkontraktionen des Anwenders angepasst werden. Bei diesem Schritt muss der Benutzer die ausgewählten Muskeln über einen Zeitraum von ein paar Sekunden so fest wie möglich anspannen, um die maximale Frequenz bzw. Leistung zu erfassen. Als nächstes werden zwei verschiedene Testphasen durchgeführt. Der erste Test (Abb.~\ref{fig:MyoTraining}(a)) dient dazu ein Bewusstsein zu entwickeln, wann und wie stark der gewünschte Muskel angespannt wird. Der grüne Punkt auf dem Display bewegt sich je nach Anspannung durch die verschiedenen Stufen und soll auch durch Entspannen wieder zur Ausgangslage gebracht werden. Bei dem zweiten Test, der in Abb.~\ref{fig:MyoTraining}(b) dargestellt ist, handelt es sich um eine zweidimensionale Überprüfung. Hier soll der Kursor in X- und Y-Richtung bewegt werden. Dies geschieht ebenfalls durch An- und Entspannung der einzelnen Muskeln. Beide Versuche bauen auf dem Trial und Error Prinzip auf. Der Benutzer muss also durch mehrere Versuche zur richtigen bzw. besten Lösung kommen. Ist der Anwender mit dem 2D-Training vertraut, kann mit der gewünschten Interaktion mit dem Computer begonnen werden. Diese ist auf dem selben Prinzip aufgebaut, allerdings gibt es unterschiedliche Oberflächen, Anordnungen der Elemente und Unterscheidungen in der Aufgabenstellung selbst \cite{MyoTraining}.
\newline \newline
%
%
\begin{figure}
\centering
\includegraphics[width=.3\textwidth]{MyoBand}
\caption{Myo Armband \cite{myoBand}.}
\label{fig:MyoBand}
\end{figure}
%
%
Abb.~\ref{fig:MyoBand} zeigt das Armband des Unternehmens Myo%
\footnote{https://www.myo.com/}
%
. Dieses verwendet die myoelektrischen Signale des Armes um durch verschiedenen Gesten mit dem Computer interagieren zu können. Es gibt fünf vordefinierte Gesten, wie z.B. das Machen einer Faust oder Wischgesten. Es können aber auch eigene Gesten definiert bzw. programmiert werden. Zum einen können die vordefinierten Gesten mit Einberechnung von Armbewegungen erweitert werden, zum anderen gibt es die Möglichkeit neue Gesten zu erstellen, da der Zugriff auf die rohen EMG-Daten möglich ist \cite{myoBand2}.%
\newline \newline \newline \newline \newline
Damit ein Computer mit Hilfe von Muskelkontraktionen gesteuert werden kann, ist ein aufwendiger und gut überlegter Algorithmus notwendig, um die erfassten Signale in das gewünschte Ergebnis zu transferieren. Der Benutzer muss vor der Interaktion einige Schritte durchlaufen, um das System an das Individuum anzupassen und um ein Bewusstsein für die Muskelsteuerung zu erlangen.


%%%%%%%%%%%%%%%%%%%%%%% Gehirnaktiviät %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Steuerung durch Gehirnaktivität}

Dieses Kapitel beschäftigt sich mit der Schnittstelle zwischen dem Gehirn und dem Computer. In der Literatur wird in diesem Zusammenhang von Brain-Computer Interface (BCI) gesprochen. Ziel ist es eine direkte Verbindung der beiden Komponenten für eine Interaktion herzustellen. Zusammengefasst liest das BCI die Wellen, die vom Gehirn an den verschiedensten Stellen produziert werden ein und übersetzt diese Signale in Befehle, damit mit einem Computer interagiert werden kann \cite{BRAIN}.
\newline \newline
Es gibt verschiedene Aufnahmetechniken von BCIs:
\begin{itemize}
      \item Invasiv
      \item Teilweise Invasiv
			\item Nicht invasiv
\end{itemize}
\vspace{\baselineskip}
Invasive Gehirn-Computer-Schnittstellen werden direkt im menschlichen Gehirn durch eine Operation eingesetzt. Es können hier einzelne oder mehrere Einheiten in verschiedenen Bereichen des Gehirns platziert und anschließend angesprochen werden. Diese Systeme weisen die höchste Qualität auf, bringen aber durch die Operation gewisse Risiken mit sich.
Bei teilweise invasiven BCI-Systemen werden die Einheiten nicht direkt in den Gehirnzellen sondern im Schädel eingesetzt und können daher die Signale etwas schlechter empfangen. Bei nicht invasiven Systemen handelt es sich um die sicherste und kostengünstigste Variante auf Kosten von schwächerem Gehirnsignalempfang. Die Signale werden über Elektroden empfangen, die am Kopf befestigt werden. Für diese Technik werden meist mit Hilfe eines Elektroenzephalografen (EEG) die neuronalen Wellen des Gehirns gemessen \cite{BRAIN}.
\newline \newline
Die neuronale Aktivität des Gehirns ist ausschlaggebend für die Messung. Daher ist es wichtig die verschiedenen Signale, die ein EEG misst kurz darzustellen \cite{BRAIN}:
\begin{itemize}
      \item Delta Signal: Hierbei handelt es sich um die langsamsten Wellen mit einer Frequenz zwischen 0.5 und 3.5 Hz, die während des Schlafens oder im komatösen Zustand auftreten.
      \item Theta Signal: Es handelt sich hierbei um Wellen, die beispielsweise während des Träumens vorkommen und werden zwischen 3.5 und 7.5 Hz. gemessen.
			\item Alpha Signal: Dies sind langsamere Wellen, die beim Problemlösen oder Entspannen auftreten und reichen von einer Frequenz zwischen 7.5 und 12 Hz.
			\item Beta Signal: Diese Wellen sind oft mit Anstrengung wie beispielsweise des Lösens einer mathematischen Aufgabe verbunden (mit einer Frequenz von 12 bis 30 Hz.).
      \item Gamma Signal: Je größer hier die Amplitude ist, desto mehr Stress, Angst oder Panik erlebt die Personen (alle Frequenzen ab 31 Hz.).
\end{itemize}
\vspace{\baselineskip}
Je nach Anwendungsbereich können verschiedene Signale verwendet werden. Für eine Interaktion mit einer Person, die sich nicht in einem komaähnlichen Zustand befindet sind alle Signale ab einer Stärke von 3.5 Hz. relevant.
\newline \newline
%
%
\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{BCI}
\caption{BCI Signalverarbeitung \cite{BRAIN}.}
\label{fig:BCI}
\end{figure}
%
%
Für den gesamte Ablauf, der nötig ist um eine Gehirn-Computer-Schnittstelle zu erstellen sind, wie in Abb.~\ref{fig:BCI} veranschaulicht, grob drei Schritte notwendig \cite{BRAIN}:
\begin{itemize}
      \item Im ersten Schritt werden die elektrischen Signale von der Kopfhaut oder der Oberfläche des Gehirns erfasst (Signalerfassung).
      \item Da die erfassten Signale in der Regel sehr schwach sind, werden diese als nächstes versärkt und von Störvariablen gereinigt. Danach werden die Signale einen Übersetzungsalgorithmus durchlaufen, damit die Absichten des Benutzers erkannt werden können (Signalverarbeitung).
			\item Im letzten Schritt müssen die Daten von einem Programm bzw. einer Software so interpretiert werden, damit diese für eine Computerinteraktion verwendet werden können.
\end{itemize}
%
%
\vspace{\baselineskip}
Um als Benutzer einen Computer mit Hilfe von Gehirnaktivität steuern zu können, muss das System vor der Interaktion individuell angepasst werden. Als mögliche Kalibrierungsmethode kann ein Mauskursor gedanklich mitverfolgt werden, während dieser sich bewegt. Der Benutzer soll sich hierbei vorstellen er würde die Bewegung mit der Hand oder dem Arm ausführen. Je nach Person zeigt sich eine Anstrengung in dem motorischen Teil des Gehirns unterschiedlich stark und das System wird dahingehend angepasst. Als nächsten Schritt wird erneut ein Mauskursor gedanklich mitverfolgt. Allerdings erscheint ein zweiter Kursor, der auf Grundlage der bereits erhobenen Daten den momentanen Stand aufzeigt. Diese zwei Schritte werden mehrmals durchgeführt, um ein optimales Ergebnis zu erzielen. Neben der Mausbewegung kann auch ein Mausklick simuliert werden. Die Auswahl eines Menüpunktes kann beispielsweise dann so funktionieren, dass ein Menüpunkt hervorgehoben wird. Will der Anwender diesen auswählen, muss es sich besonders darauf konzentrieren bzw. die Gehirnbereiche müssen eine erhöhte Aktivität vorweisen \cite{BrainInt}.
\newline \newline
Die Interaktion mit Hilfe der Messung der Gehirnaktivität ist zusammengefasst im Gegensatz zu den zuvor beschrieben Methoden eine komplexere und aufwendigere Eingabemethode. Auch die Interaktion ist mit viel Aufwand hinsichtlich der Kalibrierung verbunden.
